---
title: "PCM BT Reproducible Research"
author: "Rnewbie"
date: "February 19, 2558 BE"
output: pdf_document
---

# Installing R for Mac 
In the first paragraph, I will briefly tell you how you can install R on Mac OS. It is a very simple process because all it takes is just a few step. So, the first thing you need to do is open your web browser, and go to [CRAN](http://cran.r-project.org). It is a comprehensive R network.  You will see a number of option for you to download for different platform. We are going to download the Mac platform. Once you click R for Mac OS X you will see the latest of the files end with .pkg (latest version) and you will see the download meter will start going. It might takes a few minutes depending on your internet connection. So just be patient. Once its finish downloading, just open it up on your computer. You will see the installer which will guide you the step necessary for you to download all R. It will be on your application folder once you finish the installation process. 

# Installing RStudio for Mac 
Before installing RStudio, you **must** have R installed in your compouter. So, once you installed R, you can go [RStudio](http://www.rstudio.com). There are two version for RStudio for downloads: one is for desktop and one is for server. In here, we are not going to use server version at all. So you will just want to download the desktop the version of RStudio. Download the Mac OS version and you will see the download meter go. Once thats finished downloading, just click on it to install it. Just like any other mac application, all you have to do to install is just drag the RStudio to the application folder. You can go to application folder and double click on it. Then there you go, you are running R studio. 

# Installing R packages
Packages in R are a bunch of commands or libraries you can use to carry out certain analysis. Installing R packages in RStudio is easy. You can go to Console and type install.packages with the parasenthese and quotation mark, type in the name of the package you wish to install. In here we use multiple packages from crane as will as bioconductors. The following list of packages can be installed from crane. **e.g. install.packages("ggplot2")**. 

* unbalanced
* corrplot
* caret
* AppliedPredictiveModeling
* prospectr
* randomForest
* RCurl

For biocoundoctor, one an install the package by entering the source before the package can be installed. **e.g. source("http://bioconductor.org/biocLite.R") biocLite("Rcpi")**

* Rcpi

Once we finish installing the package necessary. we will obtained the data from the Rnewbe github page. To tackel the unbalanced data problem, we will just SMOTE algorithm from unbalanced R package. We will perform SMOTE sampling for 10 times. 

```{r}
library(RCurl)
x <- getURL("https://raw.githubusercontent.com/Rnewbie/BT/master/ROSE.csv")
ROSE <- read.csv(text=x)
library(unbalanced)
n<-ncol(ROSE)
output<-ROSE$Activity
output<-factor(output)
input<-ROSE[ ,-n]
set.seed(1)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
a <-cbind(SMOTE$X,SMOTE$Y)
#table(a$Activity)
a <- a[order(a$Activity),]
y <- a$Activity
a <- a[-1]
a <- a[-486]

set.seed(2)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
b <-cbind(SMOTE$X,SMOTE$Y)
#table(b$Activity)
b <- b[order(b$Activity),]
b <- b[-1]
b <- b[-486]

set.seed(3)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
c <-cbind(SMOTE$X,SMOTE$Y)
#table(c$Activity)
c <- c[order(c$Activity),]
c <- c[-1]
c <- c[-486]

set.seed(4)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
d <-cbind(SMOTE$X,SMOTE$Y)
#table(d$Activity)
d <- d[order(d$Activity),]
d <- d[-1]
d <- d[-486]

set.seed(5)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
e <-cbind(SMOTE$X,SMOTE$Y)
#table(e$Activity)
e <- e[order(e$Activity),]
e <- e[-1]
e <- e[-486]

set.seed(6)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
f <-cbind(SMOTE$X,SMOTE$Y)
#table(f$Activity)
f <- f[order(f$Activity),]
f <- f[-1]
f <- f[-486]

set.seed(7)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
g <-cbind(SMOTE$X,SMOTE$Y)
#table(g$Activity)
g <- g[order(g$Activity),]
g <- g[-1]
g <- g[-486]

set.seed(8)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
h <-cbind(SMOTE$X,SMOTE$Y)
#table(h$Activity)
h <- h[order(h$Activity),]
h <- h[-1]
h <- h[-486]

set.seed(9)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
i <-cbind(SMOTE$X,SMOTE$Y)
#table(i$Activity)
i <- i[order(i$Activity),]
i <- i[-1]
i <- i[-486]

set.seed(10)
SMOTE <- ubBalance(X=input, Y=output, type = "ubSMOTE",
                   perc.over = 100, perc.under = 200,
                   verbose = TRUE)
j <-cbind(SMOTE$X,SMOTE$Y)
#table(j$Activity)
j <- j[order(j$Activity),]
j <- j[-1]
j <- j[-486]
```

My teacher from my Elementray School told me, if you want to find the average value of 4 and 6, you have to add 4 and 6 and divide it into 2. 

```{r}
set.seed(1)
Jesus <- a + b + c + d + e + f + g + h + i + j
x <- Jesus/10
```

Now, lets get a perfectly balanced data for both compound and protein. 
```{r}
set.seed(1)
Activity <- getURL("https://raw.githubusercontent.com/Rnewbie/BT/master/y.csv")
Activity <- read.csv(text=Activity, header = TRUE)

set.seed(100)
compoundSMOTE <- cbind(Activity, x[, 1:87])
proteinSMOTE <- cbind(Activity, x[, 88:485])
```
Before that, we need to filter the highly correlated varialbes.So here we go,
```{r}
set.seed(3)
data <- compoundSMOTE[-1]
correlations <- cor(data, use="complete")
dim(correlations)
coor <- correlations[1:87, 1:87]
#coor
library(corrplot)
library(caret)
#corrplot(correlations, order = "hclust")
highCorr <- findCorrelation(correlations, cutoff = .70)
length(highCorr)
highCorrRemoveCompound <- data[, -highCorr]
InputKennardCompound <- cbind(Activity, highCorrRemoveCompound)
```
We also have to make for protein. I put a # on corrplot function to make R do kniting faster.
```{r}
set.seed(6)
data <- proteinSMOTE[-1]
data <- data[, -nearZeroVar(data)]
correlations <- cor(data, use="complete")
dim(correlations)
coor <- correlations[1:389, 1:389]
#coor
library(corrplot)
#corrplot(correlations, order = "hclust")
highCorr <- findCorrelation(correlations, cutoff = .70)
length(highCorr)
highCorrRemoveProtein <- data[, -highCorr]
InputKennardProtein <- cbind(Activity, highCorrRemoveProtein)
```
Now, we will do some old school Kennard Stone data Partition. Yay! Don't get so excited, remember this...When making kennary stone spliting, you have to group them according to their y output cause Kennard just blindly find maximaum distance only. 
```{r}
set.seed(300)
library(AppliedPredictiveModeling)
library(caret)
C_Inactive <- subset(InputKennardCompound, Activity == "Inactive")
C_Active <- subset(InputKennardCompound, Activity == "Active")
P_Inactive <- subset(InputKennardProtein, Activity == "Inactive")
P_Active <- subset(InputKennardProtein, Activity == "Active")
```
Finally, we will do Kennard Stone, YES.
```{r}
set.seed(6)
library(prospectr)
Inactive <- C_Inactive
x <- data.frame(Inactive)
sel <- kenStone(x[-1], k = 70, metric = "mahal", pc=2)
trainInactiveCompound <- x[sel$model, ]
testInactiveCompound <- x[sel$test, ]

Active <- C_Active
x <- data.frame(Active)
sel <- kenStone(x[-1], k = 70, metric = "mahal", pc=2)
trainActiveCompound <- x[sel$model, ]
testActiveCompound <- x[sel$test, ]

Inactive <- P_Inactive
x <- data.frame(Inactive)
sel <- kenStone(x[-1], k = 70, metric = "mahal", pc=2)
trainInactiveProtein <- x[sel$model, ]
testInactiveProtein <- x[sel$test, ]

Active <- P_Active
x <- data.frame(Active)
sel <- kenStone(x[-1], k = 70, metric = "mahal", pc=2)
trainActiveProtein <- x[sel$model, ]
testActiveProtein <- x[sel$test, ]
```
The latter part is that you have to have to combine training set and testing set for both compound and protein. For doing that, we will use rbind function from base package in R. 

```{r}
C_Train <- rbind(trainInactiveCompound, trainActiveCompound)
C_Test <- rbind(testInactiveCompound, testActiveCompound)
P_Train <- rbind(trainInactiveProtein, trainActiveProtein)
P_Test <- rbind(testInactiveProtein, testActiveProtein)
```
Before we dive into the Proteochemometrics modeling, we will do QSAR for compound and protein. We will use randomForest and caret package to perform training set, 10-fold cross validation and testing set. I will just show confusion matrix for now on as a statistical assessment. So, lets start with compound alone QSAR.

```{r}
set.seed(400)
library(randomForest)
library(caret)
Train <- C_Train
Test <- C_Test
rfModel <- randomForest(Activity ~ .,
                        data = Train,
                        ntree = 500)
```
Because we already build a random forest model with training set. It will be nice to know the performance. So we will show you the Confusion Matrix. 
```{r}
set.seed(800)
Activity <- predict(rfModel, newdata = Train)
confusionMatrix(data = Activity,
                 reference = Train$Activity,
                 positive = "Active")
```
Once we build a predictive model, we also want to perform 10-fold cross validation. Feel brave to manually perform 10 fold cross validation

```{r}
set.seed(123)
folds <- createFolds(Train$Activity, k = 10)
cv_results <- lapply(folds, function(x) {
  train <- Train[x, ]
  test <- Train[-x, ]
  model <- randomForest(Activity ~ .,
                        data = train,
                        ntree = 500)
  Prediction <- predict(model, test)
  Reference <- test$Activity
  matrix <- table(Prediction, Reference)
return(matrix)  
})
```
Now, below is the results for 10-fold cross validation. This time I will show you in details for each fold. Now what you have to do is to find average. 

```{r}
set.seed(1)
cv_results
```

Finally, we will look at the performance of testing set for compound QSAR.  
```{r}
set.seed(3)
Activity <- predict(rfModel, newdata = Test)
confusionMatrix(data = Activity,
                 reference = Test$Activity,
                 positive = "Active")
```
This is the end the QSAR for compound alone. To recap, we perform bitter taste sensing of bitter compounds using RandomForest with 500 trees. The performance of the model is assessed by the Confusion Matrix. I have described model performance of training set, 10-fold cross validation and testing set. Now, lets move on for protein descriptors QSAR.
```{r include=FALSE}
set.seed(500)
library(randomForest)
library(caret)
Train <- P_Train
Test <- P_Test
rfModel <- randomForest(Activity ~ .,
                        data = Train,
                        ntree = 500)
```
Because the codes are similar to a QSAR compound alone, i will hide them. The sequence for the presentation of the file the same.Traning set, 10-fold cv and Testing set. 

```{r echo=FALSE}
set.seed(200)
Activity <- predict(rfModel, newdata = Train)
confusionMatrix(data = Activity,
                 reference = Train$Activity,
                 positive = "Active")
```
10-fold cross validation for Protein
```{r echo=FALSE}
set.seed(123)
folds <- createFolds(Train$Activity, k = 10)
cv_results <- lapply(folds, function(x) {
  train <- Train[x, ]
  test <- Train[-x, ]
  model <- randomForest(Activity ~ .,
                        data = train,
                        ntree = 500)
  Prediction <- predict(model, test)
  Reference <- test$Activity
  matrix <- table(Prediction, Reference)
return(matrix)  
})
cv_results
```
In the end, here is the performance of testing set for protein.
```{r echo=FALSE}
set.seed(500)
Activity <- predict(rfModel, newdata = Test)
confusionMatrix(data = Activity,
                 reference = Test$Activity,
                 positive = "Active")
```
Now, lets move on to the art of performing proteochemometrics. It was developed by Wikberg along with Maris, but right now Andras Bendar is becoming the authority in this field. What makes proteochemoemtrics special is the cross-terms where interaction of multiple ligands and multiple proteins are considered. Before, performing cross terms, one should normalize the descriptors of compound and protein blocks. So we will write a function to do normalization first. 
```{r}
set.seed(1)
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```
Once we got it, we will perform crossterms function (getCPI) from Rcpi package. A huge thanks to *Nan Xian*.
```{r}
set.seed(500)
C_Train <- rbind(trainInactiveCompound, trainActiveCompound)
C_Test <- rbind(testInactiveCompound, testActiveCompound)
P_Train <- rbind(trainInactiveProtein, trainActiveProtein)
P_Test <- rbind(testInactiveProtein, testActiveProtein)
c <- C_Train
p <- P_Train
c <- c[-1]
p <- p[-1]
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
c <- as.data.frame(lapply(c, normalize))
p <- as.data.frame(lapply(p, normalize))
library(Rcpi)
crossTrain <- getCPI(c, p, type = "tensorprod")
set.seed(1)
c <- C_Test
p <- P_Test
c <- c[-1]
p <- p[-1]
c <- as.data.frame(lapply(c, normalize))
p <- as.data.frame(lapply(p, normalize))
crossTest <- getCPI(c, p, type = "tensorprod")
dim(crossTest)
```
Now, we will bind activity with predictors. 
```{r}
set.seed(1)
crossTrain <- data.frame(crossTrain)
crossTest <- data.frame(crossTest)
Activity <- P_Train$Activity
CxPTrain <- cbind(Activity, crossTrain)
Activity <- P_Test$Activity
CxPTest <- cbind(Activity, crossTest)
```
Now, we are ready to do Proteochemometrics of the Cross Terms, And again, i will hide the code too. Just like QSAR, for Proteochemoemtrics, we will do randomforest with 500 trees.
```{r include=FALSE}
set.seed(4)
library(randomForest)
library(caret)
Train <- CxPTrain
Test <- CxPTest
rfModel <- randomForest(Activity ~ .,
                        data = Train,
                        ntree = 500)
```

```{r echo=FALSE}
set.seed(700)
Activity <- predict(rfModel, newdata = Train)
confusionMatrix(data = Activity,
                 reference = Train$Activity,
                 positive = "Active")
```

```{r echo=FALSE}
set.seed(123)
folds <- createFolds(Train$Activity, k = 10)
cv_results <- lapply(folds, function(x) {
  train <- Train[x, ]
  test <- Train[-x, ]
  model <- randomForest(Activity ~ .,
                        data = train,
                        ntree = 500)
  Prediction <- predict(model, test)
  Reference <- test$Activity
  matrix <- table(Prediction, Reference)
return(matrix)  
})
cv_results
```
In the end, here is the performance of testing set for Proteochemoetmrics.
```{r}
set.seed(800)
Activity <- predict(rfModel, newdata = Test)
confusionMatrix(data = Activity,
                 reference = Test$Activity,
                 positive = "Active")
```
Now, we will perform self cross-terms which can be obtained according to this equation N * N-1 * 0.5. Because protein shakes are so delicious, we will perform pretein cross-terms. 

```{r}
set.seed(500)
C_Train <- rbind(trainInactiveCompound, trainActiveCompound)
C_Test <- rbind(testInactiveCompound, testActiveCompound)
P_Train <- rbind(trainInactiveProtein, trainActiveProtein)
P_Test <- rbind(testInactiveProtein, testActiveProtein)
c <- P_Train
p <- P_Train
c <- c[-1]
p <- p[-1]
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
c <- as.data.frame(lapply(c, normalize))
p <- as.data.frame(lapply(p, normalize))
library(Rcpi)
selfcrosstrain <- getCPI(c, p, type = "tensorprod")
IndexP <- getURL("https://raw.githubusercontent.com/Rnewbie/BT/master/IndexP.csv")
IndexP <- read.csv(text=IndexP, header = TRUE)
index <- IndexP$IndexP
IndexedPxPtrain <- selfcrosstrain[, -index]
TranposedIndexed <- t(IndexedPxPtrain)
index2 <- which(duplicated(TranposedIndexed))
removed <- TranposedIndexed[-index2, ]
finalselfcrosstrain <- t(removed)
dim(finalselfcrosstrain)
```
In here we got 79 predictors so the rule of thumb for self cross terms is N * N-1 * 0.5. In this case, it will be 79 * 78 * 0.5 = 3081. This is consistent with the above finalselfcross terms. Now, we will do the self cross terms for testing set. 
```{r}
set.seed(20)
c <- P_Test
p <- P_Test
c <- c[-1]
p <- p[-1]
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
c <- as.data.frame(lapply(c, normalize))
p <- as.data.frame(lapply(p, normalize))
library(Rcpi)
selfcrosstrain <- getCPI(c, p, type = "tensorprod")
IndexP <- getURL("https://raw.githubusercontent.com/Rnewbie/BT/master/IndexP.csv")
IndexP <- read.csv(text=IndexP, header = TRUE)
index <- IndexP$IndexP
IndexedPxPtest <- selfcrosstrain[, -index]
TranposedIndexed <- t(IndexedPxPtest)
index2 <- which(duplicated(TranposedIndexed))
removed <- TranposedIndexed[-index2, ]
finalselfcrosstest <- t(removed)
dim(finalselfcrosstest)
```
Combine Predictors with Activity
```{r}
set.seed(1)
finalselfcrosstrain <- data.frame(finalselfcrosstrain)
finalselfcrosstest <- data.frame(finalselfcrosstest)
Activity <- C_Train$Activity
PxPTrain <- cbind(Activity, finalselfcrosstrain)
Activity <- C_Test$Activity
PxPTest <- cbind(Activity, finalselfcrosstest)
```
Now, we will perform model for selfcross terms descriptors for protein. It will be the same so i will hide the code. Random Forest with 500 tress. The model assessment will be confusion matrix where training will come first, 10-fold cross validation and will end up with testing set. 

```{r include=FALSE}
set.seed(7)
library(randomForest)
library(caret)
Train <- PxPTrain
Test <- PxPTest
rfModel <- randomForest(Activity ~ .,
                        data = Train,
                        ntree = 500)
```
For training set. 
```{r echo=FALSE}
set.seed(70)
Activity <- predict(rfModel, newdata = Train)
confusionMatrix(data = Activity,
                 reference = Train$Activity,
                 positive = "Active")
```
Here is the 10-fold cross validation results for protein-protein cross terms. 
```{r echo=FALSE}
set.seed(13)
folds <- createFolds(Train$Activity, k = 10)
cv_results <- lapply(folds, function(x) {
  train <- Train[x, ]
  test <- Train[-x, ]
  model <- randomForest(Activity ~ .,
                        data = train,
                        ntree = 500)
  Prediction <- predict(model, test)
  Reference <- test$Activity
  matrix <- table(Prediction, Reference)
return(matrix)  
})
cv_results
```
In the end, here is the performance of testing set for Proteochemoetmrics, this is protein protien self cross terms.

```{r}
set.seed(800)
Activity <- predict(rfModel, newdata = Test)
confusionMatrix(data = Activity,
                 reference = Test$Activity,
                 positive = "Active")
```
Now,here is for  compound-compound cross terms. 
```{r}
set.seed(500)
C_Train <- rbind(trainInactiveCompound, trainActiveCompound)
C_Test <- rbind(testInactiveCompound, testActiveCompound)
P_Train <- rbind(trainInactiveProtein, trainActiveProtein)
P_Test <- rbind(testInactiveProtein, testActiveProtein)
c <- C_Train
p <- C_Train
c <- c[-1]
p <- p[-1]
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
c <- as.data.frame(lapply(c, normalize))
p <- as.data.frame(lapply(p, normalize))
library(Rcpi)
selfcrosstrain <- getCPI(c, p, type = "tensorprod")
IndexC <- getURL("https://raw.githubusercontent.com/Rnewbie/BT/master/IndexC.csv")
IndexC <- read.csv(text=IndexC, header = TRUE)
index <- IndexC$IndexC
IndexedPxPtrain <- selfcrosstrain[, -index]
TranposedIndexed <- t(IndexedPxPtrain)
index2 <- which(duplicated(TranposedIndexed))
removed <- TranposedIndexed[-index2, ]
finalselfcrosstrain <- t(removed)
dim(finalselfcrosstrain)
```
Now self cross terms for compound-compound test set. 
```{r}
set.seed(20)
c <- C_Test
p <- C_Test
c <- c[-1]
p <- p[-1]
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
c <- as.data.frame(lapply(c, normalize))
p <- as.data.frame(lapply(p, normalize))
library(Rcpi)
selfcrosstrain <- getCPI(c, p, type = "tensorprod")
IndexC <- getURL("https://raw.githubusercontent.com/Rnewbie/BT/master/IndexC.csv")
IndexC <- read.csv(text=IndexC, header = TRUE)
index <- IndexC$IndexC
IndexedPxPtest <- selfcrosstrain[, -index]
TranposedIndexed <- t(IndexedPxPtest)
index2 <- which(duplicated(TranposedIndexed))
removed <- TranposedIndexed[-index2, ]
finalselfcrosstest <- t(removed)
dim(finalselfcrosstest)
```
And again, combine the descriptor set with Activity.
```{r}
set.seed(1)
finalselfcrosstrain <- data.frame(finalselfcrosstrain)
finalselfcrosstest <- data.frame(finalselfcrosstest)
Activity <- C_Train$Activity
CxCTrain <- cbind(Activity, finalselfcrosstrain)
Activity <- C_Test$Activity
CxCTest <- cbind(Activity, finalselfcrosstest)
```
Now the Random forest part for compound and compound cross terms. 
```{r include=FALSE}
set.seed(7)
library(randomForest)
library(caret)
Train <- CxCTrain
Test <- CxCTest
rfModel <- randomForest(Activity ~ .,
                        data = Train,
                        ntree = 500)
```
For training set. 
```{r echo=FALSE}
set.seed(70)
Activity <- predict(rfModel, newdata = Train)
confusionMatrix(data = Activity,
                 reference = Train$Activity,
                 positive = "Active")
```
Here is the 10-fold cross validation results for compound-compound cross terms. 
```{r echo=FALSE}
set.seed(13)
folds <- createFolds(Train$Activity, k = 10)
cv_results <- lapply(folds, function(x) {
  train <- Train[x, ]
  test <- Train[-x, ]
  model <- randomForest(Activity ~ .,
                        data = train,
                        ntree = 500)
  Prediction <- predict(model, test)
  Reference <- test$Activity
  matrix <- table(Prediction, Reference)
return(matrix)  
})
cv_results
```
In the end, here is the performance of testing set for compound-compound self cross terms.
```{r}
set.seed(800)
Activity <- predict(rfModel, newdata = Test)
confusionMatrix(data = Activity,
                 reference = Test$Activity,
                 positive = "Active")
```
Thank you for following. 